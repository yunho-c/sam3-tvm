# RoPE in TVM: Investigation and Recommendations

**Date:** December 2, 2025
**Context:** Porting SAM3 Vision Backbone to Apache TVM.

## 1. Executive Summary
We investigated the validity of using `scripts/patch_rope.py` to handle Rotary Positional Embeddings (RoPE) when exporting PyTorch models to TVM. 
**Conclusion:** The patching approach is currently the **correct and recommended** method for importing standard PyTorch models that use complex numbers for RoPE. It aligns with industry practices (MLC-LLM) by effectively "lowering" complex operations to real-number arithmetic before the compiler sees them.

## 2. The Problem: Complex Numbers in TVM
Standard PyTorch RoPE implementations (like in SAM3) often use:
*   `torch.polar(abs, angle)`
*   `torch.view_as_complex`
*   Complex multiplication for rotation.

**Challenge:** 
*   Many TVM backends (e.g., OpenCL, Metal, Vulkan) and the Relax frontend have limited or experimental support for native `complex64`/`complex128` data types.
*   Directly exporting these ops often leads to "Unsupported function" errors or runtime failures on hardware that lacks native complex support.

## 3. The Solution: `scripts/patch_rope.py`
The script monkey-patches the model's RoPE logic to use explicit float arithmetic:
1.  **Replacement**: Replaces `torch.polar` with explicit `[cos, sin]` stacking (last dimension size 2).
2.  **Rotation**: Replaces complex multiplication with the rotation matrix formula:
    $$ \begin{pmatrix} x' \\ y' \end{pmatrix} = \begin{pmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} $$
    Implemented efficiently as:
    ```python
    x_out_real = x_real * cos - x_imag * sin
    x_out_imag = x_real * sin + x_imag * cos
    ```
3.  **Result**: The exported graph contains only standard `float32` ops (`mul`, `add`, `sub`, `cos`, `sin`), which are universally supported by all TVM backends.

## 4. Comparison with Standard Approaches

### A. `scripts/patch_rope.py` (Current Approach)
*   **Mechanism**: **Frontend Patching**. Manually modifies the PyTorch model's Python code *before* `torch.export` sees it.
*   **Implementation**: Replaces `complex64` operations with explicit `float32` math (stacking real/imaginary parts, manual rotation matrix application).
*   **Pros**: 
    *   **Universal Compatibility**: Works with any TVM backend (CPU, CUDA, Metal, Vulkan, OpenCL) because it produces standard `add`/`mul`/`cos`/`sin` ops.
    *   **No Compiler Changes**: Does not require modifying TVM's source code or writing custom TIR kernels.
*   **Cons**: Requires maintaining a model-specific patch script.

### B. `tvm.relax.frontend.nn` / `MLC-LLM`
*   **Mechanism**: **Compiler Abstraction**. These frameworks provide high-level APIs (e.g., `nn.Linear`, `nn.Attention`) where RoPE is a first-class citizen.
*   **Implementation**: 
    *   Uses a `RopeMode` configuration (e.g., `INLINE`, `NORMAL`, `NONE`).
    *   **`RopeMode.INLINE`**: Fuses the RoPE rotation directly into the attention kernel or previous operations.
    *   **`RopeMode.NORMAL`**: Applies RoPE as a separate kernel, often implemented in TIR using real-number arithmetic (similar to the patch, but generated by the compiler).
*   **Pros**: 
    *   **Performance**: Can generate highly optimized, fused kernels (e.g., FlashAttention with RoPE).
    *   **Cleanliness**: Model code remains clean; complexity is handled by the compiler.
*   **Cons**: 
    *   **Inflexible for Import**: Hard to apply to *existing* PyTorch models (like SAM3) without rewriting the model definition in TVM's `nn` API.

### Summary Comparison
| Feature | `scripts/patch_rope.py` | `MLC-LLM` / `tvm.relax.frontend.nn` |
| :--- | :--- | :--- |
| **Input** | Existing PyTorch Model (`torch.nn.Module`) | New Model Definition (`tvm.relax.frontend.nn.Module`) |
| **RoPE Logic** | Explicit Python `float32` math | Abstract `RopeMode` $\rightarrow$ TIR Kernel |
| **Complex Support** | Avoided via manual lowering | Avoided via compiler lowering |
| **Best For** | **Porting** existing models | **Creating** new models |

## 5. Future Outlook: The Optimal Solution
In the long term, the TVM PyTorch importer (`tvm.relax.frontend.torch`) should natively handle this, making manual patching obsolete.

**Ideal Importer Features:**
1.  **Native Complex Op Support**: Converters for `aten::polar`, `aten::view_as_complex`, `aten::mul` (complex).
2.  **Automatic Lowering**: A compiler pass that automatically decomposes these complex ops into their real-number equivalents for backends that need it.
    *   `aten::polar(r, theta)` $\rightarrow$ `concat(r*cos(theta), r*sin(theta))`
    *   `complex_mul` $\rightarrow$ Real-number rotation logic.

Until these features are standard in TVM, `scripts/patch_rope.py` remains the robust, pragmatic solution.
